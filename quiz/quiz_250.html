
<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8">
  <style>
   body
   {
       display: flex;
       justify-content: center;
       min-height: 100vh;
       background-color: #444;
       color: #fff;
   }

   .quiz_container
   {
       line-height: 2;
   }

   .mask
   {
       background-color: #fff;
       color: #fff;
       padding: 2px 8px;
       margin: 0 4px;
   }

   .mask.active
   {
       color: #000;
   }

   .btn_answer_container
   {
       text-align: center;
   }
  </style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css">
 </head>
 <body>
  <div class="quiz_container">
   <p><p><strong>Attention</strong>（<strong><span class="mask">注意機構</span></strong>）は、入力された<a href="./machine_learning_algorithm.md#%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF">時系列データ</a>のすべての<a href="#%E5%86%85%E9%83%A8%E7%8A%B6%E6%85%8B">内部状態</a>を参照し、それらに<a href="./neural_network.md#%E9%87%8D%E3%81%BF">重み</a>をつけて着目すべき部分を変化させる機構。<a href="#seq2seq">Seq2Seq</a>では、最終的な<a href="#%E5%86%85%E9%83%A8%E7%8A%B6%E6%85%8B">内部状態</a>だけを<a href="#seq2seq">Encoder</a>から<a href="#seq2seq">Decoder</a>に渡していたため、情報のボトルネックが生じてしまっていた。Attentionはこのような弱点を克服し、翻訳精度を上げることに成功している。</p>
<p>また、<a href="#rnn">RNN</a>のような前の出力を<a href="./machine_learning.md#%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB">モデル</a>に再帰的に入力するような方法では、学習を並列化させることができないため最適化に時間がかかるという欠点があったが、Attentionを用いることでこれも解決できる。</p>
</p>
   <div class="btn_answer_container">
    <button id="btn_answer" class="btn_answer">Show answer</button>
   </div>
  </div>

  <script>
   const btn_answer = document.getElementById("btn_answer");
   btn_answer.addEventListener("click", function()
   {
       const mask = document.querySelectorAll(".mask");
       mask.forEach(function(item)
       {
           item.classList.toggle("active");
       });
   });
  </script>
 </body>
</html>
            