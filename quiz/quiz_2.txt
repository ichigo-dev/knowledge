====================

最適化問題（最適化）は、目的関数が最小化あるいは最大化するような説明変数の組み合わせを求める問題。通常は目的関数の形がわかっていないため、説明変数に適当な値を入れてみてより良い値を探したり、これらの情報から目的関数の形を予測する。

説明変数の数が増えると、試行する組み合わせの数が爆発的に増加する（ ******* ）ため、最適化問題を解くための工夫を加えた最適化アルゴリズムが研究されている。

機械学習において最適なパラメータを求めることは、損失関数に関する最適化問題を解くことで、誤差を最小化するということである。損失関数は目的関数を予想して定義したものが用いられる。

Answer: 組み合わせ爆発

Source: ../../note/artificial_intelligence/_/chapters/machine_learning.md

====================

最適化問題（最適化）は、目的関数が最小化あるいは最大化するような説明変数の組み合わせを求める問題。通常は目的関数の形がわかっていないため、説明変数に適当な値を入れてみてより良い値を探したり、これらの情報から目的関数の形を予測する。

説明変数の数が増えると、試行する組み合わせの数が爆発的に増加する（組み合わせ爆発）ため、最適化問題を解くための工夫を加えた ********* が研究されている。

機械学習において最適なパラメータを求めることは、損失関数に関する最適化問題を解くことで、誤差を最小化するということである。損失関数は目的関数を予想して定義したものが用いられる。

Answer: 最適化アルゴリズム

Source: ../../note/artificial_intelligence/_/chapters/machine_learning.md

====================

 **** は、推定結果として最も尤もらしい（ふさわしい）値を求めるような方法。最もふさわしい値を求めることができる一方で、その値がどれほどふさわしいかということには注目していない。

Answer: 最尤推定

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、推測結果を値とその値の尤度によって表す方法。これにより、最もふさわしい値がどれほどふさわしいかという情報まで考慮することができる。 ***** には、ベイズの定理という確率理論が用いられる。

Answer: ベイズ推定

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** は、ベイズ推定において値がどれくらいになりそうかという予想の分布。

Answer: 事前分布

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** は、ベイズ推定を行った結果として得られる値の尤度分布。

Answer: 事後分布

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、事前分布に対して修正を加える操作。これにより最終的に事後分布が生成される。

Answer: ベイズ更新

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** は、結果となる数値と要因となる数値の関係を調べて、それぞれの関係を明らかにする統計的手法。

Answer: 回帰分析

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** は、説明変数が1つである回帰モデル。直線的にデータを予測する最もシンプルな方法で、損失関数として平方二乗誤差を使用する最小二乗法が用いられる。

Answer: 単回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

単回帰は、説明変数が1つである回帰モデル。直線的にデータを予測する最もシンプルな方法で、損失関数として平方二乗誤差を使用する ***** が用いられる。

Answer: 最小二乗法

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** は、説明変数が複数ある回帰モデル。複数の説明変数を用いることで、より複雑なモデルに対しても予測が行える。

Answer: 重回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、1つの説明変数のべき乗を組み合わせた多項式を用いる回帰モデル。次数が大きくなるほど曲線が複雑になり、不安定になってしまう。

Answer: 多項式回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ****** は、外れ値の影響を小さくするような回帰モデル。最小二乗法の外れ値に弱いという欠点を克服することを目的としている。

RANSAC(Random Sample Consensus)は ****** の代表的な方法で、データをランダムに抽出して回帰を行い、正常値に当たるデータの割合を求める。これを繰り返して最も正常値の割合が高い直線を回帰直線とする。

Answer: ロバスト回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

ロバスト回帰は、外れ値の影響を小さくするような回帰モデル。最小二乗法の外れ値に弱いという欠点を克服することを目的としている。

 ****** (Random Sample Consensus)はロバスト回帰の代表的な方法で、データをランダムに抽出して回帰を行い、正常値に当たるデータの割合を求める。これを繰り返して最も正常値の割合が高い直線を回帰直線とする。

Answer: RANSAC

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** （正規化項）は、回帰の過学習を抑えるための項で、回帰係数が大きいことによるペナルティを与える。過学習を起こした回帰曲線では、説明変数が少し変化しただけで目的変数に大きな影響を与えてしまうため、これを抑える。

Answer: 罰則項

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

罰則項（ **** ）は、回帰の過学習を抑えるための項で、回帰係数が大きいことによるペナルティを与える。過学習を起こした回帰曲線では、説明変数が少し変化しただけで目的変数に大きな影響を与えてしまうため、これを抑える。

Answer: 正規化項

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、回帰係数の絶対値の和を基準とする罰則項の設定方法。あまり重要ではない説明変数の回帰係数がゼロになる性質がある。そのため、本当に必要な変数だけが回帰に利用されることになる。

Answer: L1正則化

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、回帰係数の二乗和を基準とする罰則項の設定方法。損失関数を最小化する計算がL1正則化に比べて簡単であるが、回帰係数を正確にゼロにすることはあまりない。一般的にはL1正則化よりも ***** の方が予測の性能は高い。

Answer: L2正則化

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、L1正則化による罰則項を加えた回帰モデル。

Answer: ラッソ回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、L2正則化による罰則項を加えた回帰モデル。

Answer: リッジ回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ************* は、L1正則化とL2正則化の両方の罰則項を加えた回帰モデル。

Answer: Elastic Net回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** （サポートベクターマシン）は、データを分類する境界線（境界面）を決定するための手法。クラスごとの境界に近いデータをできるだけ境界から引き離す（マージンを最大化する）ように学習を進める。

境界は、特徴量の数が2つならば2次元平面上の直線として、特徴量の数が3つならば3次元空間上の平面として表される。特徴量が4つ以上の場合は、この境界のことを超平面という。

 *** は、直線・平面・超平面によってデータ群を分離するため、境界が曲線状になる場合には、カーネル法などを用いる必要がある。

Answer: SVM

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

SVM（ *********** ）は、データを分類する境界線（境界面）を決定するための手法。クラスごとの境界に近いデータをできるだけ境界から引き離す（マージンを最大化する）ように学習を進める。

境界は、特徴量の数が2つならば2次元平面上の直線として、特徴量の数が3つならば3次元空間上の平面として表される。特徴量が4つ以上の場合は、この境界のことを超平面という。

SVMは、直線・平面・超平面によってデータ群を分離するため、境界が曲線状になる場合には、カーネル法などを用いる必要がある。

Answer: サポートベクターマシン

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** は、SVMにおいて、境界線（境界面）から最も近いデータとの距離。

Answer: マージン

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** （分離 *** ）は、特徴量が4つ以上のデータに対するSVMにおいて、各クラスのデータを分離するための境界。

Answer: 超平面

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

超平面（ ***** ）は、特徴量が4つ以上のデータに対するSVMにおいて、各クラスのデータを分離するための境界。

Answer: 分離超平面

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********** は、直線・平面・超平面によって完全にデータを分離するSVM。

Answer: ハードマージンSVM

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********** は、直線・平面・超平面に対して誤差を認めるSVMで、マージン上にデータが存在することを許容する。マージンに入ったデータにはペナルティを与え、マージンの最大化とペナルティの最小化を目的として最適化を進める。

Answer: ソフトマージンSVM

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、線形分離不可能なデータ群を線形分離可能な高次元特徴空間に写像することで、SVMを適用する方法。元の低次元空間では線形分離ができない場合に、それを線形分離可能な高次元特徴空間に写像してSVMを適用し、それを元の低次元空間に逆写像することで境界を求める。

Answer: カーネル法

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ****** は、カーネル法において、低次元空間のデータ群を高次元特徴空間に写像するための変換関数。

Answer: カーネル関数

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** は、カーネル法においてデータを高次元特徴空間に写像する際に、計算が複雑にならないように式変形を行うテクニック。

Answer: カーネルトリック

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** （サポートベクトル回帰）は、SVMを回帰問題に応用したもの。ソフトマージンSVMにおけるマージンの最大化とペナルティの最小化を用いて、正規化最小二乗法を使った回帰に置き替えることで、回帰直線を求める。

Answer: SVR

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

SVR（ ********** ）は、SVMを回帰問題に応用したもの。ソフトマージンSVMにおけるマージンの最大化とペナルティの最小化を用いて、正規化最小二乗法を使った回帰に置き替えることで、回帰直線を求める。

Answer: サポートベクトル回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** は、木構造を用いて条件分岐を繰り返していき分類を行う手法。各ノードには条件が設けれており、末端のノードは分類されるクラスにあたる。モデルがブラックボックス化しないため解析がしやすく、データの前処理がほとんど必要ないという特徴がある。

Answer: 決定木

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ** は、決定木の過学習を防止するための方法。分割の深さを制限したり、分割に必要なデータの数の下限を定めたりする。訓練データによって決定木を意図的に過学習させたのちに、検証データを使って性能の悪い決定木の分岐を切り取ることで、過学習を防いで予測能力を向上させる、といった方法がある。

Answer: 剪定

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** (Reduced Error Pruning)は、剪定したノードを最も割合の高い葉ノードに置き替える方法。

Answer: REP

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** は、精度の低いモデルを複数組み合わせることで精度の高いモデルを作る方法。弱学習器は精度は低いものの学習にかかる時間が短い。このような弱学習器の出力を全て参考にし、多数決や平均、加重平均などによって最終的な出力を決定する。

Answer: アンサンブル学習

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********* は、母集団から重複込みでランダムにデータを抽出する方法。

Answer: ブートストラップ法

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** は、ブートストラップ法によって全データから訓練データを複数組生成し、それぞれのデータ群に対してモデルを用意するアンサンブル学習の手法。ランダムに訓練データを生成することで、それぞれのモデルが影響を受けるノイズが打ち消しあう。

Answer: バギング

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******* は、バギングと類似したアンサンブル学習の手法で、訓練データの組を生成する際に、復元抽出ではなく非復元抽出を用いる。

Answer: ペースティング

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******* は、モデルを順番に学習させていき、前のモデルの出力結果と実際の値との差を補正するように次のモデルを学習させるアンサンブル学習の手法。並行してモデルの学習ができないため、バギングよりも時間がかかる。

Answer: ブースティング

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********* は、決定木を用いたバギングの一種で、決定木を分岐させるときに使う特徴量もランダムに抽出する手法。各モデルの決定木がどれも同じ特徴量についての分岐ばかりでは予測精度の向上が見込めないため、異なる特徴量が利用されるようにする。

Answer: ランダムフォレスト

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ****** は、最初にバギングと同様ブートストラップ法で得たデータを各モデルに学習させ、そのモデルの予測結果を入力として次のモデルを学習させる手法。アンサンブル学習の各モデルのうち、どの出力がより有効であるかを2段階目以降のモデルに学習させる構造となる。

Answer: スタッキング

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********* は、ブースティングを行う際に、前のモデルの予測値と正解データの誤差を最小化するために勾配降下法を用いる手法。新しいモデルは古いモデルの欠点を穴埋めするように学習されていく。

Answer: 勾配ブースティング

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********* は、回帰分析と同様、関数の最適な係数を発見する手法で、主に分類に用いられる。ここで、関数としてはロジスティック関数が用いられ、この関数の値はどちらのクラスに分類されるかを表す確率となる。

Answer: ロジスティック回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******* （ロジスティック関数）は、最小値が $0$ で最大値が $1$ となるようなS字曲線の関数。

Answer: シグモイド関数

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

シグモイド関数（ ********* ）は、最小値が $0$ で最大値が $1$ となるようなS字曲線の関数。

Answer: ロジスティック関数

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** は、ベイズ推定によってデータがどのように発生しているのかという発生構造をモデル化する手法。

Answer: ベイジアンモデル

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** は、時系列データに対するモデリングを行い分析をするような手法。基本的には、現在の値は少し前の値に近いという前提でモデリングを行う。

Answer: 時系列分析

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ****** は、あるデータの状態が、そのデータの過去の状態に影響を受けて決定されるようなデータ群。 ****** のように、データがそのデータ自身に影響を受けるような性質のことを自己相関があるという。

Answer: 時系列データ

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

時系列データは、あるデータの状態が、そのデータの過去の状態に影響を受けて決定されるようなデータ群。時系列データのように、データがそのデータ自身に影響を受けるような性質のことを **** があるという。

Answer: 自己相関

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** (AR)は、最も基本的な時系列モデルで、単回帰や重回帰に自身の過去の値による補正項を加えたもの。

Answer: 自己回帰

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

自己回帰( ** )は、最も基本的な時系列モデルで、単回帰や重回帰に自身の過去の値による補正項を加えたもの。

Answer: AR

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** (MA)は、過去の値を利用した **** によって未来の値を予測するモデル。

Answer: 移動平均

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

移動平均( ** )は、過去の値を利用した移動平均によって未来の値を予測するモデル。

Answer: MA

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** (ARMA)は、自己回帰と移動平均を組み合わせたモデルで、より現実に則したものとなっている。

Answer: 自己回帰移動平均

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

自己回帰移動平均( **** )は、自己回帰と移動平均を組み合わせたモデルで、より現実に則したものとなっている。

Answer: ARMA

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********** (ARIMA)は、自己回帰移動平均モデルに、上昇傾向や下降傾向といったトレンドのあるデータの予測に対応したもの。

Answer: 自己回帰和分移動平均

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

自己回帰和分移動平均( ***** )は、自己回帰移動平均モデルに、上昇傾向や下降傾向といったトレンドのあるデータの予測に対応したもの。

Answer: ARIMA

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ************ (SARIMA)は、自己回帰和分移動平均モデルに、周期的な値の変動（季節変動）を考慮させたもの。

Answer: 季節自己回帰和分移動平均

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

季節自己回帰和分移動平均( ****** )は、自己回帰和分移動平均モデルに、周期的な値の変動（季節変動）を考慮させたもの。

Answer: SARIMA

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******* は、実際に観測される値と、その裏に隠れた本当の状態との間の観測誤差を考慮した時系列分析のモデル。観測モデルと状態モデルを組み合わせてモデル化を行う。

Answer: 状態空間モデル

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 **** は、主に教師あり学習の分類に利用される手法で、分類したいデータと各クラスのベクトルの類似度を計算する方法。類似度には、2つのベクトル間の距離が用いられることが多い。

Answer: k近傍法

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** （L1ノルム）は、2点間の座標平面（空間）上の軸に沿った最短の道なり。

Answer: マンハッタン距離

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

マンハッタン距離（ ***** ）は、2点間の座標平面（空間）上の軸に沿った最短の道なり。

Answer: L1ノルム

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** （L2ノルム）は、2点間の直線距離。k近傍法において用いられる、最も一般的な類似度である。

Answer: ユークリッド距離

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

ユークリッド距離（ ***** ）は、2点間の直線距離。k近傍法において用いられる、最も一般的な類似度である。

Answer: L2ノルム

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** は、2点間の座標平面（空間）上の軸に沿った差のうち最大のもの。

Answer: チェビシェフ距離

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ********* は、2点間の距離の指標を一般化したもの。2点 $x, y$ 間の距離 $D(x, y)$ は、 $D(x, y) = (\sum^{n}_{i=0}{| x_i - y_i |^p})^{\frac{1}{p}}$ で表され、 $p=1$ のときはマンハッタン距離、 $p=2$ の時はユークリッド距離、 $p=\infty$ の時はチェビシェフ距離となる。

Answer: ミンコフスキー距離

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******** （k平均法）は、教師なし学習におけるクラスタリングの手法で、データをベクトル化したときの距離に応じてそのデータが所属するクラスタを決定する。

Answer: k-means法

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

k-means法（ **** ）は、教師なし学習におけるクラスタリングの手法で、データをベクトル化したときの距離に応じてそのデータが所属するクラスタを決定する。

Answer: k平均法

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

** ********* **は、k-means法を改良した方法で、k-means法が初期にランダムにクラスタの割り当てを行うことで、性能が乱数に依存する問題を軽減している。

Answer: k-means++

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ***** (PCA)は、次元削減に用いられる手法で、データの情報（分散）をなるべく損なわないようにして、複数の特徴量から新しい特徴量を合成する。この手法で合成された新しい特徴量のことを主成分といい、分散が最大化されるような主成分を第一主成分、次に分散が大きくなる主成分を第二主成分という。

Answer: 主成分分析

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

主成分分析( *** )は、次元削減に用いられる手法で、データの情報（分散）をなるべく損なわないようにして、複数の特徴量から新しい特徴量を合成する。この手法で合成された新しい特徴量のことを主成分といい、分散が最大化されるような主成分を第一主成分、次に分散が大きくなる主成分を第二主成分という。

Answer: PCA

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 *** 分析(PCA)は、次元削減に用いられる手法で、データの情報（分散）をなるべく損なわないようにして、複数の特徴量から新しい特徴量を合成する。この手法で合成された新しい特徴量のことを *** といい、分散が最大化されるような *** を第一 *** 、次に分散が大きくなる *** を第二 *** という。

Answer: 主成分

Source: ../../note/artificial_intelligence/_/chapters/machine_learning_algorithm.md

====================

 ******* は、単一のニューロンをモデル化したもの。 ******* では、複数の入力に対してそれぞれ重みをかけたものの和をとる。また、この時バイアスを足し合わせる場合もある。こうして求められた和に対して、活性化関数と呼ばれる非線形関数を適用し、最終的な出力を求める。

Answer: パーセプトロン

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ***** は、人間の神経回路。

Answer: ニューロン

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *********** (NN: Neural Network)は、人間の脳の神経回路を模した学習モデルで、パーセプトロンを連結した構造となっている。

Answer: ニューラルネットワーク

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

ニューラルネットワーク( ** : Neural Network)は、人間の脳の神経回路を模した学習モデルで、パーセプトロンを連結した構造となっている。

Answer: NN

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** は、ニューラルネットワークにおける、単体のパーセプトロン。

Answer: ノード

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** は、ニューラルネットワークにおける、ノードとノードを接続している部分。各 *** はそれぞれ重みを持っている。

Answer: エッジ

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ** は、ニューラルネットワークにおけるパラメータで、ノードとノードの結びつきの強さ。 ** が大きいほど後段のノードへの影響が大きくなる。

Answer: 重み

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 **** は、ニューラルネットワークにおけるパラメータで、各ノードで入力に関係なく加えられる項。

Answer: バイアス

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** は、ニューラルネットワークに入力される特徴量を受け付けるレイヤ。

Answer: 入力層

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** （隠れ層）は、ニューラルネットワークにおいて、入力層と出力層の間にあるレイヤで、データを分類したり分析したりといった様々な操作を行う。ネットワークごとに様々な種類の *** のレイヤが用意される。

Answer: 中間層

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

中間層（ *** ）は、ニューラルネットワークにおいて、入力層と出力層の間にあるレイヤで、データを分類したり分析したりといった様々な操作を行う。ネットワークごとに様々な種類の中間層のレイヤが用意される。

Answer: 隠れ層

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** は、ニューラルネットワークにおいて、最終的なデータの分類・分析結果を出力するレイヤ。

Answer: 出力層

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ************* は、中間層を1層持つ、最も基本的なニューラルネットワーク。

Answer: 3層ニューラルネットワーク

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ********* (DL: Deep Learning)は、中間層の階層が深い（入力層と出力層を含めて3階層よりレイヤが多い）ニューラルネットワークを用いた学習方法。隠れ層やノードの数が増えたことにより、複雑な出力を行うことができるようになる。

Answer: ディープラーニング

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

ディープラーニング( ** : Deep Learning)は、中間層の階層が深い（入力層と出力層を含めて3階層よりレイヤが多い）ニューラルネットワークを用いた学習方法。隠れ層やノードの数が増えたことにより、複雑な出力を行うことができるようになる。

Answer: DL

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** は、ニューラルネットワークに入力されたデータが、重みを掛けられたり、活性化関数を適用されたりすることで、最終的な結果が出力されるという流れ。ニューラルネットワークにより予測や分類を行うには *** を用いる。

Answer: 順伝搬

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ****** （バックプロバゲーション）は、ニューラルネットワークの出力と正解データとの誤差を、出力層から入力層に向かって伝搬（逆伝搬）しながらパラメータを調整する方法。出力層の損失関数の値が小さくなるように、各ノードでの局所誤差を小さくしながらモデルを最適化していく。

Answer: 誤差逆伝搬法

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

誤差逆伝搬法（ *********** ）は、ニューラルネットワークの出力と正解データとの誤差を、出力層から入力層に向かって伝搬（逆伝搬）しながらパラメータを調整する方法。出力層の損失関数の値が小さくなるように、各ノードでの局所誤差を小さくしながらモデルを最適化していく。

Answer: バックプロバゲーション

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

誤差 *** 法（バックプロバゲーション）は、ニューラルネットワークの出力と正解データとの誤差を、出力層から入力層に向かって伝搬（ *** ）しながらパラメータを調整する方法。出力層の損失関数の値が小さくなるように、各ノードでの局所誤差を小さくしながらモデルを最適化していく。

Answer: 逆伝搬

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ***** は、最適化アルゴリズムのひとつで、ある説明変数が入力された時の目的関数の値とその傾きを用いて、目的関数の値が最小化されるように傾きの下降方向にパラメータを調整する方法。勾配（傾き）は、ある説明変数が入力されたときの目的関数の微分係数で求められる。

 ***** は、初期値によって、必ずしも大域最適解に向かってパラメータを調整するとは限らず、局所最適解に陥ってしまうこともある。

 ***** においては、学習率の設定が非常に重要となる。学習率が小さすぎるとパラメータが収束するまでに時間がかかってしまい、学習率が大きすぎるとパラメータが発散して正しい値に収束しない。

Answer: 勾配降下法

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ** 降下法は、最適化アルゴリズムのひとつで、ある説明変数が入力された時の目的関数の値とその傾きを用いて、目的関数の値が最小化されるように傾きの下降方向にパラメータを調整する方法。 ** （傾き）は、ある説明変数が入力されたときの目的関数の微分係数で求められる。

 ** 降下法は、初期値によって、必ずしも大域最適解に向かってパラメータを調整するとは限らず、局所最適解に陥ってしまうこともある。

 ** 降下法においては、学習率の設定が非常に重要となる。学習率が小さすぎるとパラメータが収束するまでに時間がかかってしまい、学習率が大きすぎるとパラメータが発散して正しい値に収束しない。

Answer: 勾配

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ***** は、説明変数がとり得る範囲全体で、目的関数が最小化（または最大化）されるような説明変数の組み合わせ。

Answer: 大域最適解

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 ***** は、ある限られた範囲において、目的関数が最小化（または最大化）されるような説明変数の組み合わせ。

Answer: 局所最適解

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

 *** （確率的勾配降下法）は、誤差関数に用いるデータをランダムに選択することで目的関数の形を微妙に変化させ、局所最適解に陥ることを防ぐ手法。学習率の設定が難しく、収束が遅い。

Answer: SGD

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

====================

SGD（ ******** ）は、誤差関数に用いるデータをランダムに選択することで目的関数の形を微妙に変化させ、局所最適解に陥ることを防ぐ手法。学習率の設定が難しく、収束が遅い。

Answer: 確率的勾配降下法

Source: ../../note/artificial_intelligence/_/chapters/neural_network.md

