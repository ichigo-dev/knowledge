# 『ニューラルネットワークアルゴリズム』

（最終更新： 2023-02-13）


## 目次

1. [全結合層](#全結合層)
1. [CNN](#cnn)
	1. [畳み込み層](#畳み込み層)
	1. [プーリング層](#プーリング層)
1. [RNN](#rnn)


## 全結合層

**全結合層**は、ニューラルネットワークに用いられるレイヤのひとつで、前の層の出力全てと結合するようなノードからなる。


## CNN

**CNN**（**畳み込みニューラルネットワーク**）は、平面・空間上で隣り合う特徴量を考慮して学習を行う機構を持ったニューラルネットワーク。

### 畳み込み層

**畳み込み層**は、畳み込みフィルタによって平面・空間上のある範囲に反応するフィルタを用いて新しい**特徴マップ**を作るレイヤ。データの特徴量の端の部分からフィルタを適用し、フィルタをずらしていくことで特徴マップを作成する。

### プーリング層

**プーリング層**は、平面・空間上のある範囲（ウィンドウ）のうち値を1つだけ抽出することで特徴量を削減するレイヤ。**マックスプーリング**では、ウィンドウ上で最大の値だけを抽出する。


## RNN

**RNN**（**再帰型ニューラルネットワーク**）は、時系列データを再帰的に処理する機構を持ったニューラルネットワーク。ニューラルネットワークがループ状に接続されるような構造となっており、古いデータを処理したときの情報を次のデータを処理する際に引き継ぐ。

### 再帰セル

**再帰セル**は、RNNのモデル中でループで繋がれているレイヤ。

### 内部状態

**内部状態**（隠れ状態）は、再帰セルが保持する情報。

### LSTM

**LSTM**(Long Short Term Memory)は、再帰セルの構造のひとつで、古い情報の記憶が薄れていく（内部情報には直帰のデータの状態が反映されやすく、最初のデータの情報は徐々に消えていく）という再帰セルの弱点を軽減している。また、時間方向の勾配消失問題を軽減し、学習が効率よく進む。LSTMには情報の伝わり方を調整するための3つの**ゲート**が設けられている。

**忘却ゲート**では前の情報をどれだけ切り捨てるかを調整し、**入力ゲート**では新しい情報をどれだけ取り込むかを調整し、**出力ゲート**では情報をどれだけ出力するかを調整する。

### GRU

**GRU**(Gated Recurrent Unit)は、再帰セルの構造のひとつで、LSTMの構造を単純化している。GRUには情報の伝わり方を調整するための2つの**ゲート**が設けられている。

**リセットゲート**では情報をどれだけ切り捨てるかを調整し、**更新ゲート**では情報をどれだけ取り込むかを調整する。

### 双方向RNN

**双方向RNN**は、前のデータの情報だけでなく、後ろのデータの情報も用いることで予測精度を向上させたRNN。

### Seq2Seq

**Seq2Seq**(sequence-to-sequence)は、自然言語処理に特化した双方向RNNモデル。

ある単語系列をRNN(**Encoder**)に入力し、Encoderの最終的な内部状態を別のRNN(**Decoder**)に入力として渡すことで、新しい単語系列を出力させる。これにより機械翻訳などを実現している。


## Attention

**Attention**（**注意機構**）は、入力された時系列データのすべての内部状態を参照し、それらに重みをつけて着目すべき部分を変化させる機構。Seq2Seqでは、最終的な内部状態だけをEncoderからDecoderに渡していたため、情報のボトルネックが生じてしまっていた。Attentionはこのような弱点を克服し、翻訳精度を上げることに成功している。


## Transformer

**Transformer**は、Attentionを応用した**Self-Attention**機構を持つニューラルネットワークのモデル。Self-Attentionは自然言語処理のタスクにおいて、ある単語がその文章中のどの単語と結びつきが強いのか、という情報を明らかにするレイヤ。基本形はSeq2Seqと同様Encoder-Decoderのモデルであるが、Encoderのみを取り出したものもある。
