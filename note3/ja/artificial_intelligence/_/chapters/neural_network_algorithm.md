# 『ニューラルネットワークアルゴリズム』

（最終更新： 2023-02-14）


## 目次

1. [全結合層](#全結合層)
1. [CNN](#cnn)
	1. [畳み込み層](#畳み込み層)
	1. [プーリング層](#プーリング層)
1. [RNN](#rnn)
	1. [再帰セル](#再帰セル)
	1. [内部状態](#内部状態)
	1. [LSTM](#lstm)
	1. [GRU](#gru)
	1. [双方向RNN](#双方向rnn)
	1. [Seq2Seq](#seq2seq)
	1. [ELMo](#elmo)
1. [Attention](#attention)
1. [Transformer](#transformer)
1. [BERT](#bert)


## 全結合層

**全結合層**は、ニューラルネットワークに用いられるレイヤのひとつで、前の層の出力全てと結合するようなノードからなる。


## CNN

**CNN**（**畳み込みニューラルネットワーク**）は、平面・空間上で隣り合う特徴量を考慮して学習を行う機構を持ったニューラルネットワーク。

### 畳み込み層

**畳み込み層**は、畳み込みフィルタによって平面・空間上のある範囲に反応するフィルタを用いて新しい**特徴マップ**を作るレイヤ。データの特徴量の端の部分からフィルタを適用し、フィルタをずらしていくことで特徴マップを作成する。

### プーリング層

**プーリング層**は、平面・空間上のある範囲（ウィンドウ）のうち値を1つだけ抽出することで特徴量を削減するレイヤ。**マックスプーリング**では、ウィンドウ上で最大の値だけを抽出する。


## RNN

**RNN**（**再帰型ニューラルネットワーク**）は、時系列データを再帰的に処理する機構を持ったニューラルネットワーク。ニューラルネットワークがループ状に接続されるような構造となっており、古いデータを処理したときの情報を次のデータを処理する際に引き継ぐ。

### 再帰セル

**再帰セル**は、RNNのモデル中でループで繋がれているレイヤ。

### 内部状態

**内部状態**（隠れ状態）は、再帰セルが保持する情報。

### LSTM

**LSTM**(Long Short Term Memory)は、再帰セルの構造のひとつで、古い情報の記憶が薄れていく（内部情報には直帰のデータの状態が反映されやすく、最初のデータの情報は徐々に消えていく）という再帰セルの弱点を軽減している。また、時間方向の勾配消失問題を軽減し、学習が効率よく進む。LSTMには情報の伝わり方を調整するための3つの**ゲート**が設けられている。

**忘却ゲート**では前の情報をどれだけ切り捨てるかを調整し、**入力ゲート**では新しい情報をどれだけ取り込むかを調整し、**出力ゲート**では情報をどれだけ出力するかを調整する。

### GRU

**GRU**(Gated Recurrent Unit)は、再帰セルの構造のひとつで、LSTMの構造を単純化している。GRUには情報の伝わり方を調整するための2つの**ゲート**が設けられている。

**リセットゲート**では情報をどれだけ切り捨てるかを調整し、**更新ゲート**では情報をどれだけ取り込むかを調整する。

### 双方向RNN

**双方向RNN**は、前のデータの情報だけでなく、後ろのデータの情報も用いることで予測精度を向上させたRNN。

### Seq2Seq

**Seq2Seq**(sequence-to-sequence)は、自然言語処理に特化した双方向RNNモデル。

ある単語系列をRNN(**Encoder**)に入力し、Encoderの最終的な内部状態を別のRNN(**Decoder**)に入力として渡すことで、新しい単語系列を出力させる。これにより機械翻訳などを実現している。

### ELMo

**ELMo**(Embeddings from Language Models)は、双方向LSTMを用いた自然言語処理のモデルで、ある単語をその前後の文脈を考慮してベクトルに変換する。純粋な分散表現では、ひとつの単語をそのままベクトルとして表現するため、文脈の違いや多義語に対する認識性能が低いという欠点があった。


## Attention

**Attention**（**注意機構**）は、入力された時系列データのすべての内部状態を参照し、それらに重みをつけて着目すべき部分を変化させる機構。Seq2Seqでは、最終的な内部状態だけをEncoderからDecoderに渡していたため、情報のボトルネックが生じてしまっていた。Attentionはこのような弱点を克服し、翻訳精度を上げることに成功している。

また、RNNのような前の出力をモデルに再帰的に入力するような方法では、学習を並列化させることができないため最適化に時間がかかるという欠点があったが、Attentionを用いることでこれも解決できる。


## Transformer

**Transformer**は、Attentionを応用した**Self-Attention**機構を持つニューラルネットワークのモデル。Self-Attentionは自然言語処理のタスクにおいて、ある単語がその文章中のどの単語と結びつきが強いのか、という情報を明らかにするレイヤ。基本形はSeq2Seqと同様Encoder-Decoderのモデルであるが、Encoderのみを取り出したものもある。


## BERT

**BERT**(Bidirectional Encoder Representations from Transformers)は、Googleが開発した自然言語処理のモデルで、Transformerを用いて双方向に単語をエンコードする。双方向RNNやELMoと同様、前後の文脈から単語の意味を推測することができる。


## オートエンコーダ

**オートエンコーダ**（**自己符号化器**）は、教師なし学習の手法のひとつで、ニューラルネットワークの入力と同じ出力を行うようにモデルを学習する。中間層のサイズを入力層よりも小さくしておくことで、学習後のモデルの中間層には入力された特徴量を圧縮した情報が存在する状態となる。最終的に出力層（あるいは後段の層）を取り除くことで、ニューラルネットワークを特徴抽出器として利用することができる。

### VAE

**VAE**（**変分自己符号化器**: Variational Autoencoder）は、オートエンコーダにおいて中間層の潜在変数に確率分布を用いることで、未知のデータに対しても確率的に応用できる。
