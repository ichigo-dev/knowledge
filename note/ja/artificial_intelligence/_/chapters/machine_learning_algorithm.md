# 『機械学習アルゴリズム』ノート

（最終更新： 2023-05-09）


## 目次

1. [最尤推定](#最尤推定)
1. [ベイズ推定](#ベイズ推定)
	1. [事前分布](#事前分布)
	1. [事後分布](#事後分布)
	1. [ベイズ更新](#ベイズ更新)
1. [回帰分析](#回帰分析)
	1. [単回帰](#単回帰)
	1. [重回帰](#重回帰)
	1. [多項式回帰](#多項式回帰)
	1. [ロバスト回帰](#ロバスト回帰)
	1. [罰則項](#罰則項)
	1. [L1正則化](#l1正則化)
	1. [L2正則化](#l2正則化)
	1. [ラッソ回帰](#ラッソ回帰)
	1. [リッジ回帰](#リッジ回帰)
	1. [Elastic Net回帰](#elastic-net回帰)
1. [SVM](#svm)
	1. [マージン](#マージン)
	1. [超平面](#超平面)
	1. [ハードマージンSVM](#ハードマージンsvm)
	1. [ソフトマージンSVM](#ソフトマージンsvm)
	1. [カーネル法](#カーネル法)
	1. [カーネル関数](#カーネル関数)
	1. [カーネルトリック](#カーネルトリック)
	1. [SVR](#svr)
1. [決定木](#決定木)
	1. [剪定](#剪定)
	1. [REP](#rep)
1. [アンサンブル学習](#アンサンブル学習)
	1. [ブートストラップ法](#ブートストラップ法)
	1. [バギング](#バギング)
	1. [ペースティング](#ペースティング)
	1. [ブースティング](#ブースティング)
	1. [ランダムフォレスト](#ランダムフォレスト)
	1. [スタッキング](#スタッキング)
	1. [勾配ブースティング](#勾配ブースティング)
1. [ロジスティック回帰](#ロジスティック回帰)
	1. [シグモイド関数](#シグモイド関数)
1. [ベイジアンモデル](#ベイジアンモデル)
1. [時系列分析](#時系列分析)
	1. [時系列データ](#時系列データ)
	1. [自己回帰](#自己回帰)
	1. [移動平均](#移動平均)
	1. [自己回帰移動平均](#自己回帰移動平均)
	1. [自己回帰和分移動平均](#自己回帰和分移動平均)
	1. [季節自己回帰和分移動平均](#季節自己回帰和分移動平均)
	1. [状態空間モデル](#状態空間モデル)
1. [k近傍法](#k近傍法)
	1. [マンハッタン距離](#マンハッタン距離)
	1. [ユークリッド距離](#ユークリッド距離)
	1. [チェビシェフ距離](#チェビシェフ距離)
	1. [ミンコフスキー距離](#ミンコフスキー距離)
1. [k-means法](#k-means法)
	1. [k-means++](#k-means)
1. [主成分分析](#主成分分析)


## 最尤推定

**最尤推定**は、[推定](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#推定)結果として最も尤もらしい（ふさわしい）値を求めるような方法。最もふさわしい値を求めることができる一方で、その値がどれほどふさわしいかということには注目していない。


## ベイズ推定

**ベイズ推定**は、推測結果を値とその値の尤度によって表す方法。これにより、最もふさわしい値がどれほどふさわしいかという情報まで考慮することができる。ベイズ推定には、[ベイズの定理](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#ベイズの定理)という[確率](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#確率)理論が用いられる。

### 事前分布

**事前分布**は、[ベイズ推定](#ベイズ推定)において値がどれくらいになりそうかという予想の分布。

### 事後分布

**事後分布**は、[ベイズ推定](#ベイズ推定)を行った結果として得られる値の尤度分布。

### ベイズ更新

**ベイズ更新**は、[事前分布](#事前分布)に対して修正を加える操作。これにより最終的に[事後分布](#事後分布)が生成される。


## 回帰分析

**回帰分析**は、結果となる数値と要因となる数値の関係を調べて、それぞれの関係を明らかにする統計的手法。

### 単回帰

**単回帰**は、[説明変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)が1つである[回帰](./machine_learning.md#回帰)モデル。直線的にデータを予測する最もシンプルな方法で、[損失関数](./machine_learning.md#損失関数)として[平方二乗誤差](./machine_learning.md#平方二乗誤差)を使用する**最小二乗法**が用いられる。

### 重回帰

**重回帰**は、[説明変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)が複数ある[回帰](./machine_learning.md#回帰)モデル。複数の[説明変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)を用いることで、より複雑なモデルに対しても予測が行える。

### 多項式回帰

**多項式回帰**は、1つの[説明変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)のべき乗を組み合わせた多項式を用いる[回帰](#回帰分析)モデル。次数が大きくなるほど曲線が複雑になり、不安定になってしまう。

### ロバスト回帰

**ロバスト回帰**は、外れ値の影響を小さくするような[回帰](./machine_learning.md#回帰)モデル。最小二乗法の外れ値に弱いという欠点を克服することを目的としている。

**RANSAC**(Random Sample Consensus)はロバスト回帰の代表的な方法で、データをランダムに抽出して[回帰](./machine_learning.md#回帰)を行い、正常値に当たるデータの割合を求める。これを繰り返して最も正常値の割合が高い直線を回帰直線とする。

### 罰則項

**罰則項**（**正規化項**）は、[回帰](./machine_learning.md#回帰)の[過学習](./machine_learning.md#過学習)を抑えるための項で、回帰係数が大きいことによるペナルティを与える。[過学習](./machine_learning.md#過学習)を起こした回帰曲線では、[説明変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)が少し変化しただけで[目的変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)に大きな影響を与えてしまうため、これを抑える。

### L1正則化

**L1正則化**は、回帰係数の絶対値の和を基準とする[罰則項](#罰則項)の設定方法。あまり重要ではない[説明変数](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#回帰分析)の回帰係数がゼロになる性質がある。そのため、本当に必要な変数だけが[回帰](./machine_learning.md#回帰)に利用されることになる。

### L2正則化

**L2正則化**は、回帰係数の二乗和を基準とする[罰則項](#罰則項)の設定方法。[損失関数](./machine_learning.md#損失関数)を最小化する計算が[L1正則化](#l1正則化)に比べて簡単であるが、回帰係数を正確にゼロにすることはあまりない。一般的には[L1正則化](#l1正則化)よりもL2正則化の方が予測の性能は高い。

### ラッソ回帰

**ラッソ回帰**は、[L1正則化](#l1正則化)による[罰則項](#罰則項)を加えた[回帰](./machine_learning.md#回帰)モデル。

### リッジ回帰

**リッジ回帰**は、[L2正則化](#l2正則化)による[罰則項](#罰則項)を加えた[回帰](./machine_learning.md#回帰)モデル。

### Elastic Net回帰

**Elastic Net回帰**は、[L1正則化](#l1正則化)と[L2正則化](#l2正則化)の両方の[罰則項](#罰則項)を加えた[回帰](./machine_learning.md#回帰)モデル。


## SVM

**SVM**（**サポートベクターマシン**）は、データを分類する境界線（境界面）を決定するための手法。[クラス](./machine_learning.md#クラス)ごとの境界に近いデータをできるだけ境界から引き離す（[マージン](#マージン)を最大化する）ように学習を進める。

境界は、[特徴量](./machine_learning.md#特徴量)の数が2つならば2次元平面上の直線として、[特徴量](./machine_learning.md#特徴量)の数が3つならば3次元空間上の平面として表される。[特徴量](./machine_learning.md#特徴量)が4つ以上の場合は、この境界のことを[超平面](#超平面)という。

SVMは、直線・平面・超平面によってデータ群を分離するため、境界が曲線状になる場合には、[カーネル法](#カーネル法)などを用いる必要がある。

### マージン

**マージン**は、[SVM](#svm)において、境界線（境界面）から最も近いデータとの距離。

### 超平面

**超平面**（**分離超平面**）は、[特徴量](./machine_learning.md#特徴量)が4つ以上のデータに対する[SVM](#svm)において、各[クラス](./machine_learning.md#クラス)のデータを分離するための境界。

### ハードマージンSVM

**ハードマージンSVM**は、直線・平面・[超平面](#超平面)によって完全にデータを分離する[SVM](#svm)。

### ソフトマージンSVM

**ソフトマージンSVM**は、直線・平面・[超平面](#超平面)に対して誤差を認める[SVM](#svm)で、[マージン](#マージン)上にデータが存在することを許容する。[マージン](#マージン)に入ったデータにはペナルティを与え、[マージン](#マージン)の最大化とペナルティの最小化を目的として最適化を進める。

### カーネル法

**カーネル法**は、線形分離不可能なデータ群を線形分離可能な高次元特徴空間に写像することで、[SVM](#svm)を適用する方法。元の低次元空間では線形分離ができない場合に、それを線形分離可能な高次元特徴空間に写像して[SVM](#svm)を適用し、それを元の低次元空間に逆写像することで境界を求める。

### カーネル関数

**カーネル関数**は、[カーネル法](#カーネル法)において、低次元空間のデータ群を高次元特徴空間に写像するための変換関数。

### カーネルトリック

**カーネルトリック**は、[カーネル法](#カーネル法)においてデータを高次元特徴空間に写像する際に、計算が複雑にならないように式変形を行うテクニック。

### SVR

**SVR**（**サポートベクトル回帰**）は、[SVM](#svm)を[回帰](./machine_learning.md#回帰)問題に応用したもの。[ソフトマージンSVM](#ソフトマージンsvm)における[マージン](#マージン)の最大化とペナルティの最小化を用いて、正規化最小二乗法を使った[回帰](./machine_learning.md#回帰)に置き替えることで、回帰直線を求める。


## 決定木

**決定木**は、[木](../../../basics/applied_mathematics/_/chapters/graph_theory.md#木)構造を用いて条件分岐を繰り返していき[分類](./machine_learning.md#分類)を行う手法。各[ノード](../../../basics/applied_mathematics/_/chapters/graph_theory.md#グラフ)には条件が設けれており、末端の[ノード](../../../basics/applied_mathematics/_/chapters/graph_theory.md#グラフ)は分類される[クラス](./machine_learning.md#クラス)にあたる。[モデル](./machine_learning.md#学習モデル)がブラックボックス化しないため解析がしやすく、データの[前処理](./machine_learning.md#前処理)がほとんど必要ないという特徴がある。

### 剪定

**剪定**は、[決定木](#決定木)の[過学習](./machine_learning.md#過学習)を防止するための方法。分割の深さを制限したり、分割に必要なデータの数の下限を定めたりする。[訓練データ](./machine_learning.md#訓練データ)によって[決定木](#決定木)を意図的に[過学習](./machine_learning.md#過学習)させたのちに、[検証データ](./machine_learning.md#検証データ)を使って性能の悪い[決定木](#決定木)の分岐を切り取ることで、[過学習](./machine_learning.md#過学習)を防いで予測能力を向上させる、といった方法がある。

### REP

**REP**(Reduced Error Pruning)は、[剪定](#剪定)した[ノード](../../../basics/applied_mathematics/_/chapters/graph_theory.md#グラフ)を最も割合の高い葉[ノード](../../../basics/applied_mathematics/_/chapters/graph_theory.md#グラフ)に置き替える方法。


## アンサンブル学習

**アンサンブル学習**は、精度の低い[モデル](./machine_learning.md#学習モデル)を複数組み合わせることで精度の高い[モデル](./machine_learning.md#学習モデル)を作る方法。弱学習器は精度は低いものの学習にかかる時間が短い。このような弱学習器の出力を全て参考にし、多数決や[平均](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#平均値)、加重平均などによって最終的な出力を決定する。

### ブートストラップ法

**ブートストラップ法**は、[母集団](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#推定)から重複込みでランダムにデータを抽出する方法。

### バギング

**バギング**は、[ブートストラップ法](#ブートストラップ法)によって全データから[訓練データ](./machine_learning.md#訓練データ)を複数組生成し、それぞれのデータ群に対して[モデル](./machine_learning.md#学習モデル)を用意する[アンサンブル学習](#アンサンブル学習)の手法。ランダムに[訓練データ](./machine_learning.md#訓練データ)を生成することで、それぞれの[モデル](./machine_learning.md#学習モデル)が影響を受けるノイズが打ち消しあう。

### ペースティング

**ペースティング**は、[バギング](#バギング)と類似した[アンサンブル学習](#アンサンブル学習)の手法で、[訓練データ](./machine_learning.md#訓練データ)の組を生成する際に、復元抽出ではなく非復元抽出を用いる。

### ブースティング

**ブースティング**は、[モデル](./machine_learning.md#学習モデル)を順番に学習させていき、前の[モデル](./machine_learning.md#学習モデル)の出力結果と実際の値との差を補正するように次の[モデル](./machine_learning.md#学習モデル)を学習させる[アンサンブル学習](#アンサンブル学習)の手法。並行して[モデル](./machine_learning.md#学習モデル)の学習ができないため、[バギング](#バギング)よりも時間がかかる。

### ランダムフォレスト

**ランダムフォレスト**は、[決定木](#決定木)を用いた[バギング](#バギング)の一種で、[決定木](#決定木)を分岐させるときに使う[特徴量](./machine_learning.md#特徴量)もランダムに抽出する手法。各[モデル](./machine_learning.md#学習モデル)の[決定木](#決定木)がどれも同じ[特徴量](./machine_learning.md#特徴量)についての分岐ばかりでは予測精度の向上が見込めないため、異なる[特徴量](./machine_learning.md#特徴量)が利用されるようにする。

### スタッキング

**スタッキング**は、最初に[バギング](#バギング)と同様[ブートストラップ法](#ブートストラップ法)で得たデータを各[モデル](./machine_learning.md#学習モデル)に学習させ、その[モデル](./machine_learning.md#学習モデル)の予測結果を入力として次の[モデル](./machine_learning.md#学習モデル)を学習させる手法。[アンサンブル学習](#アンサンブル学習)の各[モデル](./machine_learning.md#学習モデル)のうち、どの出力がより有効であるかを2段階目以降の[モデル](./machine_learning.md#学習モデル)に学習させる構造となる。

### 勾配ブースティング

**勾配ブースティング**は、[ブースティング](#ブースティング)を行う際に、前の[モデル](./machine_learning.md#学習モデル)の予測値と正解データの誤差を最小化するために[勾配降下法](./neural_network.md#勾配降下法)を用いる手法。新しい[モデル](./machine_learning.md#学習モデル)は古い[モデル](./machine_learning.md#学習モデル)の欠点を穴埋めするように学習されていく。


## ロジスティック回帰

**ロジスティック回帰**は、[回帰分析](#回帰分析)と同様、関数の最適な係数を発見する手法で、主に[分類](./machine_learning.md#分類)に用いられる。ここで、関数としては[ロジスティック関数](#シグモイド関数)が用いられ、この関数の値はどちらの[クラス](./machine_learning.md#クラス)に分類されるかを表す[確率](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#確率)となる。

### シグモイド関数

**シグモイド関数**（**ロジスティック関数**）は、最小値が $0$ で最大値が $1$ となるようなS字曲線の関数。


## ベイジアンモデル

**ベイジアンモデル**は、[ベイズ推定](#ベイズ推定)によってデータがどのように発生しているのかという発生構造をモデル化する手法。


## 時系列分析

**時系列分析**は、[時系列データ](#時系列データ)に対するモデリングを行い分析をするような手法。基本的には、現在の値は少し前の値に近いという前提でモデリングを行う。

### 時系列データ

**時系列データ**は、あるデータの状態が、そのデータの過去の状態に影響を受けて決定されるようなデータ群。時系列データのように、データがそのデータ自身に影響を受けるような性質のことを**自己相関**があるという。

### 自己回帰

**自己回帰**(**AR**)は、最も基本的な時系列モデルで、[単回帰](#単回帰)や[重回帰](#重回帰)に自身の過去の値による補正項を加えたもの。

### 移動平均

**移動平均**(**MA**)は、過去の値を利用した移動平均によって未来の値を予測する[モデル](./machine_learning.md#学習モデル)。

### 自己回帰移動平均

**自己回帰移動平均**(**ARMA**)は、[自己回帰](#自己回帰)と[移動平均](#移動平均)を組み合わせた[モデル](./machine_learning.md#学習モデル)で、より現実に則したものとなっている。

### 自己回帰和分移動平均

**自己回帰和分移動平均**(**ARIMA**)は、[自己回帰移動平均](#自己回帰移動平均)[モデル](./machine_learning.md#学習モデル)に、上昇傾向や下降傾向といったトレンドのあるデータの予測に対応したもの。

### 季節自己回帰和分移動平均

**季節自己回帰和分移動平均**(**SARIMA**)は、[自己回帰和分移動平均](#自己回帰和分移動平均)[モデル](./machine_learning.md#学習モデル)に、周期的な値の変動（季節変動）を考慮させたもの。

### 状態空間モデル

**状態空間モデル**は、実際に観測される値と、その裏に隠れた本当の状態との間の観測誤差を考慮した時系列分析の[モデル](./machine_learning.md#学習モデル)。観測モデルと状態モデルを組み合わせてモデル化を行う。


## k近傍法

**k近傍法**は、主に[教師あり学習](./machine_learning.md#教師あり学習)の分類に利用される手法で、分類したいデータと各[クラス](./machine_learning.md#クラス)の[ベクトル](../../../basics/applied_mathematics/_/chapters/numerical_calculation.md#ベクトル)の類似度を計算する方法。類似度には、2つの[ベクトル](../../../basics/applied_mathematics/_/chapters/numerical_calculation.md#ベクトル)間の距離が用いられることが多い。

### マンハッタン距離

**マンハッタン距離**（**L1ノルム**）は、2点間の座標平面（空間）上の軸に沿った最短の道なり。

### ユークリッド距離

**ユークリッド距離**（**L2ノルム**）は、2点間の直線距離。k近傍法において用いられる、最も一般的な類似度である。

### チェビシェフ距離

**チェビシェフ距離**は、2点間の座標平面（空間）上の軸に沿った差のうち最大のもの。

### ミンコフスキー距離

**ミンコフスキー距離**は、2点間の距離の指標を一般化したもの。2点 $x, y$ 間の距離 $D(x, y)$ は、 $D(x, y) = (\sum^{n}_{i=0}{| x_i - y_i |^p})^{\frac{1}{p}}$ で表され、 $p=1$ のときは[マンハッタン距離](#マンハッタン距離)、 $p=2$ の時は[ユークリッド距離](#ユークリッド距離)、 $p=\infty$ の時は[チェビシェフ距離](#チェビシェフ距離)となる。


## k-means法

**k-means法**（**k平均法**）は、[教師なし学習](./machine_learning.md#教師なし学習)における[クラスタリング](./machine_learning.md#クラスタリング)の手法で、データを[ベクトル](../../../basics/applied_mathematics/_/chapters/numerical_calculation.md#ベクトル)化したときの距離に応じてそのデータが所属するクラスタを決定する。

### k-means++

**k-means++**は、[k-means法](#k-means法)を改良した方法で、[k-means法](#k-means法)が初期にランダムにクラスタの割り当てを行うことで、性能が乱数に依存する問題を軽減している。


## 主成分分析

**主成分分析**(**PCA**)は、[次元削減](./machine_learning.md#次元削減)に用いられる手法で、データの情報（[分散](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#分散)）をなるべく損なわないようにして、複数の[特徴量](./machine_learning.md#特徴量)から新しい[特徴量](./machine_learning.md#特徴量)を合成する。この手法で合成された新しい[特徴量](./machine_learning.md#特徴量)のことを**主成分**といい、[分散](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#分散)が最大化されるような主成分を第一主成分、次に[分散](../../../basics/applied_mathematics/_/chapters/probability_and_statistics.md#分散)が大きくなる主成分を第二主成分という。
