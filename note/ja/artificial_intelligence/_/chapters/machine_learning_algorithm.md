# 『機械学習アルゴリズム』ノート

（最終更新： 2023-03-03）


## 目次

1. [最尤推定](#最尤推定)
1. [ベイズ推定](#ベイズ推定)
	1. [事前分布](#事前分布)
	1. [事後分布](#事後分布)
	1. [ベイズ更新](#ベイズ更新)
1. [回帰分析](#回帰分析)
	1. [単回帰](#単回帰)
	1. [重回帰](#重回帰)
	1. [多項式回帰](#多項式回帰)
	1. [ロバスト回帰](#ロバスト回帰)
	1. [罰則項](#罰則項)
	1. [L1正則化](#l1正則化)
	1. [L2正則化](#l2正則化)
	1. [ラッソ回帰](#ラッソ回帰)
	1. [リッジ回帰](#リッジ回帰)
	1. [Elastic Net回帰](#elastic-net回帰)
1. [SVM](#svm)
	1. [マージン](#マージン)
	1. [超平面](#超平面)
	1. [ハードマージンSVM](#ハードマージンsvm)
	1. [ソフトマージンSVM](#ソフトマージンsvm)
	1. [カーネル法](#カーネル法)
	1. [カーネル関数](#カーネル関数)
	1. [カーネルトリック](#カーネルトリック)
	1. [SVR](#svr)
1. [決定木](#決定木)
	1. [剪定](#剪定)
	1. [REP](#rep)
1. [アンサンブル学習](#アンサンブル学習)
	1. [ブートストラップ法](#ブートストラップ法)
	1. [バギング](#バギング)
	1. [ペースティング](#ペースティング)
	1. [ブースティング](#ブースティング)
	1. [ランダムフォレスト](#ランダムフォレスト)
	1. [スタッキング](#スタッキング)
	1. [勾配ブースティング](#勾配ブースティング)
1. [ロジスティック回帰](#ロジスティック回帰)
	1. [シグモイド関数](#シグモイド関数)
1. [ベイジアンモデル](#ベイジアンモデル)
1. [時系列分析](#時系列分析)
	1. [時系列データ](#時系列データ)
	1. [自己回帰](#自己回帰)
	1. [移動平均](#移動平均)
	1. [自己回帰移動平均](#自己回帰移動平均)
	1. [自己回帰和分移動平均](#自己回帰和分移動平均)
	1. [季節自己回帰和分移動平均](#季節自己回帰和分移動平均)
	1. [状態空間モデル](#状態空間モデル)
1. [k近傍法](#k近傍法)
	1. [マンハッタン距離](#マンハッタン距離)
	1. [ユークリッド距離](#ユークリッド距離)
	1. [チェビシェフ距離](#チェビシェフ距離)
	1. [ミンコフスキー距離](#ミンコフスキー距離)
1. [k-means法](#k-means法)
	1. [k-means++](#k-means)
1. [主成分分析](#主成分分析)


## 最尤推定

**最尤推定**は、推定結果として最も尤もらしい（ふさわしい）値を求めるような方法。最もふさわしい値を求めることができる一方で、その値がどれほどふさわしいかということには注目していない。


## ベイズ推定

**ベイズ推定**は、推測結果を値とその値の尤度によって表す方法。これにより、最もふさわしい値がどれほどふさわしいかという情報まで考慮することができる。ベイズ推定には、ベイズの定理という確率理論が用いられる。

### 事前分布

**事前分布**は、ベイズ推定において値がどれくらいになりそうかという予想。

### 事後分布

**事後分布**は、ベイズ推定を行った結果として得られる値の尤度分布。

### ベイズ更新

**ベイズ更新**は、事前分布に対して修正を加える操作。これにより最終的に事後分布が生成される。


## 回帰分析

**回帰分析**は、結果となる数値と要因となる数値の関係を調べて、それぞれの関係を明らかにする統計的手法。

### 単回帰

**単回帰**は、説明変数が1つである回帰モデル。直線的にデータを予測する最もシンプルな方法で、損失関数として平方二乗誤差を使用する**最小二乗法**が用いられる。

### 重回帰

**重回帰**は、説明変数が複数ある回帰モデル。複数の説明変数を用いることで、より複雑なモデルに対しても予測が行える。

### 多項式回帰

**多項式回帰**は、1つの説明変数のべき乗を組み合わせた多項式を用いる回帰モデル。字数が大きくなるほど曲線が複雑になり、不安定になってしまう。

### ロバスト回帰

**ロバスト回帰**は、外れ値の影響を小さくするような回帰モデル。最小二乗法の外れ値に弱いという欠点を克服することを目的としている。

**RANSAC**(Random Sample Consensus)はロバスト回帰の代表的な方法で、データをランダムに抽出して回帰を行い、正常値に当たるデータの割合を求める。これを繰り返して最も正常値の割合が高い直線を回帰直線とする。

### 罰則項

**罰則項**（**正規化項**）は、回帰の過学習を抑えるための項で、回帰係数が大きいことによるペナルティを与える。過学習を起こした回帰曲線では、説明変数が少し変化しただけで目的変数に大きな影響を与えてしまうため、これを抑える。

### L1正則化

**L1正則化**は、回帰係数の絶対値の和を基準とする罰則項の設定方法。あまり重要ではない説明変数の回帰係数がゼロになる性質がある。そのため、本当に必要な変数だけが回帰に利用されることになる。

### L2正則化

**L2正則化**は、回帰係数の二乗和を基準とする罰則項の設定方法。損失関数を最小化する計算がL1正則化に比べて簡単であるが、回帰係数を正確にゼロにすることはあまりない。一般的にはL1正則化よりもL2正則化の方が予測の性能は高い。

### ラッソ回帰

**ラッソ回帰**は、L1正則化による罰則項を加えた回帰モデル。

### リッジ回帰

**リッジ回帰**は、L2正則化による罰則項を加えた回帰モデル。

### Elastic Net回帰

**Elastic Net回帰**は、L1正則化とL2正則化の両方の罰則項を加えた回帰モデル。


## SVM

**SVM**（**サポートベクターマシン**）は、データを分類する境界線（境界面）を決定するための手法。クラスごとの境界に近いデータをできるだけ境界から引き離す（マージンを最大化する）ように学習を進める。

境界は、特徴量の数が2つならば2次元平面上の直線として、特徴量の数が3つならば3次元空間上の平面として表される。特徴量が4つ以上の場合は、この境界のことを超平面という。

SVMは、直線・平面・超平面によってデータ群を分離するため、境界が曲線状になる場合には、カーネル法などを用いる必要がある。

### マージン

**マージン**は、SVMにおいて、境界線（境界面）から最も近いデータとの距離。

### 超平面

**超平面**（**分離超平面**）は、特徴量が4つ以上のデータに対するSVMにおいて、各クラスのデータを分離するための境界。

### ハードマージンSVM

**ハードマージンSVM**は、直線・平面・超平面によって完全にデータを分離するSVM。

### ソフトマージンSVM

**ソフトマージンSVM**は、直線・平面・超平面に対して誤差を認めるSVMで、マージン上にデータが存在することを許容する。マージンに入ったデータにはペナルティを与え、マージンの最大化とペナルティの最小化を目的賭して最適化を進める。

### カーネル法

**カーネル法**は、線形分離不可能なデータ群を線形分離可能な高次元特徴空間に写像することで、SVMを適用する方法。元の低次元空間では線形分離ができない場合に、それを線形分離可能な高次元特徴空間に写像してSVMを適用し、それを元の低次元空間に逆写像することで境界を求める。

### カーネル関数

**カーネル関数**は、カーネル法において、低次元空間のデータ群を高次元特徴空間に写像するための変換関数。

### カーネルトリック

**カーネルトリック**は、カーネル法においてデータを高次元特徴空間に写像する際に、計算が複雑にならないように式変形を行うテクニックのこと。

### SVR

**SVR**（**サポートベクトル回帰**）は、SVMを回帰問題に応用したもの。ソフトマージンSVMにおけるマージンの最大化とペナルティの最小化を用いて、正規化最小二乗法を使った回帰に置き替えることで、回帰直線を求める。


## 決定木

**決定木**は、木構造を用いて条件分岐を繰り返していき分類を行う手法。各ノードには条件が設けれており、末端のノードは分類されるクラスにあたる。モデルがブラックボックス化しないため解析がしやすく、データの前処理がほとんど必要ないという特徴がある。

### 剪定

**剪定**は、決定木の過学習を防止するための方法。分割の深さを制限したり、分割に必要なデータの数の下限を定めたりする。訓練データによって決定木を意図的に過学習させたのちに、検証データを使って性能の悪い決定木の分岐を切り取ることで、過学習を防いで予測能力を向上させる。

### REP

**REP**(Reduced Error Pruning)は、剪定したノードを最も割合の高い葉ノードに置き替える方法。


## アンサンブル学習

**アンサンブル学習**は、精度の低いモデルを複数組み合わせることで精度の高いモデルを作る方法。弱学習器は精度は低いものの学習にかかる時間が短い。このような弱学習器の出力を全て参考にし、多数決や平均、加重平均などによって最終的な出力を決定する。

### ブートストラップ法

**ブートストラップ法**は、母集団から重複込みでランダムにデータを抽出する方法。

### バギング

**バギング**は、ブートストラップ法によって全データから訓練データを複数組生成し、それぞれのデータ群に対してモデルを用意するアンサンブル学習の手法。ランダムに訓練データを生成することで、それぞれのモデルが影響を受けるノイズが打ち消しあう。

### ペースティング

**ペースティング**は、バギングと類似したアンサンブル学習の手法で、訓練データの組を生成する際に、復元抽出ではなく非復元抽出を用いる。

### ブースティング

**ブースティング**は、モデルを順番に学習させていき、前のモデルの出力結果と実際の値との差を補正するように次のモデルを学習させるアンサンブル学習の手法。並行してモデルの学習ができないため、バギングよりも時間がかかる。

### ランダムフォレスト

**ランダムフォレスト**は、決定木を用いたバギングの一種で、決定木を分岐させるときに使う特徴量もランダムに抽出する手法。各モデルの決定木がどれも同じ特徴量についての分岐ばかりでは予測精度の向上が見込めないため、異なる特徴量が利用されるようにする。

### スタッキング

**スタッキング**は、最初にバギングと同様ブートストラップ法で得たデータを各モデルに学習させ、そのモデルの予測結果を入力として次のモデルを学習させる手法。アンサンブル学習の各モデルのうち、どの出力がより有効であるかを2段階目以降のモデルに学習させる構造となる。

### 勾配ブースティング

**勾配ブースティング**は、ブースティングを行う際に、前のモデルの予測値と正解データの誤差を最小化するために勾配降下法を用いる手法。新しいモデルは古いモデルの欠点を穴埋めするように学習されていく。


## ロジスティック回帰

**ロジスティック回帰**は、回帰分析と同様、関数の最適な係数を発見する手法で、主に分類に用いられる。ここで、関数としてはロジスティック関数が用いられ、この関数の値はどちらのクラスに分類されるかを表す確率となる。

### シグモイド関数

**シグモイド関数**（**ロジスティック関数**）は、最小値が0で最大値が1となるようなS字曲線の関数。


## ベイジアンモデル

**ベイジアンモデル**は、ベイズ推定によってデータがどのように発生しているのかという発生構造をモデル化する手法。


## 時系列分析

**時系列分析**は、時系列データに対するモデリングを行い分析をするような手法。基本的には、現在の値は少し前の値に近いという前提でモデリングを行う。

### 時系列データ

**時系列データ**は、あるデータの状態が、そのデータの過去の状態に影響を受けて決定されるようなデータ群。時系列データのように、データがそのデータ自身に影響を受けるような性質のことを**自己相関**があるという。

### 自己回帰

**自己回帰**(**AR**)は、最も基本的な時系列モデルで、単回帰や重回帰に自身の過去の値による補正項を加えたもの。

### 移動平均

**移動平均**(**MA**)は、過去の値を利用した移動平均によって未来の値を予測するモデル。

### 自己回帰移動平均

**自己回帰移動平均**(**ARMA**)は、自己回帰と移動平均を組み合わせたモデルで、より現実に則したモデルとなっている。

### 自己回帰和分移動平均

**自己回帰和分移動平均**(**ARIMA**)は、自己回帰移動平均モデルに、上昇傾向や下降傾向といったトレンドのあるデータの予測に対応したモデル。

### 季節自己回帰和分移動平均

**季節自己回帰和分移動平均**(**SARIMA**)は、自己回帰和分移動平均モデルに、周期的な値の変動（季節変動）を考慮させたモデル。

### 状態空間モデル

**状態空間モデル**は、実際に観測される値と、その裏に隠れた本当の状態との間の観測誤差を考慮した時系列分析のモデル。観測モデルと状態モデルを組み合わせてモデル化を行う。


## k近傍法

**k近傍法**は、主に教師あり学習の分類に利用される手法で、分類したいデータと各クラスのベクトルの類似度を計算する方法。類似度には、2つのベクトル間の距離が用いられることが多い。

### マンハッタン距離

**マンハッタン距離**（**L1ノルム**）は、2点間の座標平面（空間）上の軸に沿った最短の道なり。

### ユークリッド距離

**ユークリッド距離**（**L2ノルム**）は、2点間の直線距離。k近傍法において用いられる、最も一般的な類似度である。

### チェビシェフ距離

**チェビシェフ距離**は、2点間の座標平面（空間）上の軸に沿った差のうち最大のもの。

### ミンコフスキー距離

**ミンコフスキー距離**は、2点間の距離の指標を一般化したもの。2点 $x, y$ 間の距離 $D(x, y)$ は、 $D(x, y) = (\sum^{n}_{i=0}{| x_i - y_i |^p})^{\frac{1}{p}}$ で表され、 $p=1$ のときはマンハッタン距離、 $p=2$ の時はユークリッド距離、 $p=\infty$ の時はチェビシェフ距離となる。


## k-means法

**k-means法**（**k平均法**）は、教師なし学習におけるクラスタリングの手法で、データをベクトル化したときの距離に応じてそのデータが所属するクラスタを決定する。

### k-means++

**k-means++**は、k-means法を改良した方法で、k-means法が初期にランダムにクラスタの割り当てを行うことで、性能が乱数に依存する問題を軽減している。


## 主成分分析

**主成分分析**(**PCA**)は、次元削減に用いられる手法で、データの情報（分散）をなるべく損なわないようにして、複数の特徴量から新しい特徴量を合成する。この手法で合成された新しい特徴量のことを**主成分**といい、分散が最大化されるような主成分を第一主成分、次に分散が大きくなる主成分を第二主成分という。
