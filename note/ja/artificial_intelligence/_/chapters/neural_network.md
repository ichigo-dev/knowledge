# 『ニューラルネットワーク』ノート

（最終更新： 2023-03-03）


## 目次

1. [パーセプトロン](#パーセプトロン)
	1. [ニューロン](#ニューロン)
1. [ニューラルネットワーク](#ニューラルネットワーク)
	1. [ノード](#ノード)
	1. [エッジ](#エッジ)
	1. [重み](#重み)
	1. [バイアス](#バイアス)
	1. [入力層](#入力層)
	1. [中間層](#中間層)
	1. [出力層](#出力層)
	1. [3層ニューラルネットワーク](#3層ニューラルネットワーク)
	1. [ディープラーニング](#ディープラーニング)
1. [順伝搬](#順伝搬)
1. [誤差逆伝搬法](#誤差逆伝搬法)
1. [勾配降下法](#勾配降下法)
	1. [大域最適解](#大域最適解)
	1. [局所最適解](#局所最適解)
	1. [SGD](#sgd)
	1. [モーメンタム](#モーメンタム)
	1. [Adagrad](#adagrad)
	1. [RMSprop](#rmsprop)
	1. [Adam](#adam)
1. [勾配消失問題](#勾配消失問題)
1. [勾配爆発問題](#勾配爆発問題)
1. [活性化関数](#活性化関数)
	1. [ステップ関数](#ステップ関数)
	1. [シグモイド関数](#シグモイド関数)
	1. [tanh関数](#tanh関数)
	1. [ReLU関数](#relu関数)
1. [ドロップアウト](#ドロップアウト)
1. [転移学習](#転移学習)
	1. [ファインチューニング](#ファインチューニング)
	1. [負の転移](#負の転移)
	1. [ドメイン混合](#ドメイン混合)
	1. [マルチタスク学習](#マルチタスク学習)
	1. [One-shot学習](#one-shot学習)
	1. [Zero-shot学習](#zero-shot学習)
1. [パターン認識](#パターン認識)
1. [自然言語処理](#自然言語処理)
	1. [分散表現](#分散表現)


## パーセプトロン

**パーセプトロン**は、単一のニューロンをモデル化したもの。パーセプトロンでは、複数の入力に対してそれぞれ重みをかけたものの和をとる。また、この時**バイアス**を足し合わせる場合もある。こうして求められた和に対して、活性化関数と呼ばれる非線形関数を適用し、最終的な出力を求める。

### ニューロン

**ニューロン**は、人間の神経回路。


## ニューラルネットワーク

**ニューラルネットワーク**(**NN**: Neural Network)は、人間の脳の神経回路を模した学習モデルで、パーセプトロンを連結した構造となっている。

### ノード

**ノード**は、ニューラルネットワークにおいて、単体のパーセプトロンを指す。

### エッジ

**エッジ**は、ニューラルネットワークにおいて、ノードとノードを接続している部分を指す。エッジは重みを持っている。

### 重み

**重み**は、ニューラルネットワークにおけるパラメータで、ノードとノードの結びつきの強さを表す。重みが大きいほど後段のノードへの影響が大きくなる。

### バイアス

**バイアス**は、ニューラルネットワークにおけるパラメータで、各ノードで入力に関係なく加えられる項のこと。

### 入力層

**入力層**は、ニューラルネットワークに入力される特徴量を受け付けるレイヤ。

### 中間層

**中間層**（**隠れ層**）は、ニューラルネットワークにおいて入力層と出力層の間にあるレイヤで、データを分類したり分析したり、その他にも様々な操作を行う。ネットワークごとに様々な種類の中間層のレイヤが用意される。

### 出力層

**出力層**は、ニューラルネットワークによるデータの分類・分析結果を出力するレイヤ。

### 3層ニューラルネットワーク

**3層ニューラルネットワーク**は、中間層を1層持つ、最も基本的なニューラルネットワーク。

### ディープラーニング

**ディープラーニング**(**DL**: Deep Learning)は、中間層の階層が深い（3階層以上）のニューラルネットワークを用いた学習方法。隠れ層やノードの数が増えたことにより、複雑な出力を行うことができるようになる。


## 順伝搬

**順伝搬**は、ニューラルネットワークに入力されたデータが、重みを掛けられたり、活性化関数を適用されたりすることで、最終的な結果が出力される流れ。ニューラルネットワークにより予測や分類を行うには順伝搬を用いる。


## 誤差逆伝搬法

**誤差逆伝搬法**（**バックプロバゲーション**）は、ニューラルネットワークの出力と正解データとの誤差を、出力層から入力層に向かって伝搬（**逆伝搬**）しながらパラメータを調整する方法。出力層の損失関数の値が小さくなるように、各ノードでの局所誤差を小さくしながらモデルを最適化していく。


## 勾配降下法

**勾配降下法**は、最適化アルゴリズムのひとつで、ある説明変数が入力された時の目的関数の値とその傾きを用いて、目的関数の値が最小化されるように傾きの下降方向にパラメータを調整する方法。**勾配**（傾き）は、ある説明変数が入力されたときの目的関数の微分係数で求められる。

勾配降下法は、初期値によって、必ずしも帯域最適解に向かってパラメータを調整するとは限らず、局所最適解に陥ってしまうこともある。

勾配降下法においては、学習率の設定が非常に重要となる。学習率が小さすぎるとパラメータが収束するまでに時間がかかってしまい、学習率が大きすぎるとパラメータが発散して正しい値に収束しない。

### 大域最適解

**大域最適解**は、説明変数がとり得る範囲全体で、目的関数が最小化（または最大化）されるような説明変数の組み合わせ。

### 局所最適解

**局所最適解**は、ある限られた範囲において、目的関数が最小化（または最大化）されるような説明変数の組み合わせ。

### SGD

**SGD**（**確率的勾配降下法**）は、誤差関数に用いるデータをランダムに選択することで目的関数の形を微妙に変化させ、局所最適解に陥ることを防ぐ手法。学習率の設定が難しく、収束が遅い。

### モーメンタム

**モーメンタム**は、SGDに完成の概念を加えたアルゴリズムで、直前の勾配を考慮した項を加えることで収束までの時間を短縮することができる。

### Adagrad

**Adagrad**は、SGDにおいて学習が進むにつれて学習率を自動的に小さくしていくアルゴリズムで、パラメータが発散することを防ぐ。

### RMSprop

**RMSprop**は、Adagradを改良したアルゴリズムで、勾配の合計を指数移動平均することで、より最近の勾配を重視することができる。

### Adam

**Adam**は、RMSpropとモーメンタムを組み合わせたアルゴリズムで、それぞれの恩恵を受けることができる。


## 勾配消失問題

**勾配消失問題**は、誤差逆伝搬法によってパラメータを更新する際に、レイヤを通過するたびに勾配が小さくなっていき、前段の層のパラメータの収束に時間がかかる問題。勾配は各ノードにおける活性化関数の微分により算出できるが、この値が1よりも小さい場合は徐々に勾配が小さくなっていく。

シグモイド関数を微分した関数は最大値が0.25と小さく、勾配消失問題が発生しやすいため、隠れ層の活性化関数としてはReLU関数がよく用いられている。


## 勾配爆発問題

**勾配爆発問題**は、誤差逆伝搬法によってパラメータを更新する際に、レイヤを通過するたびに勾配が大きくなっていき、前段の層のパラメータの更新時に発散してしまう問題。


## 活性化関数

**活性化関数**は、入力を重み付けした和を別の値に変形させる関数のことで、非線形関数が用いられる。活性化関数を用いないパーセプトロンやニューラルネットワークは、出力が各層の重みづけを行った入力の線形関数で表現されるため、線形分離可能な問題にしか対応できない。

### ステップ関数

**ステップ関数**は、入力が0より大きければ1、0以下であれば0を出力するような関数。初期のパーセプトロンの活性化関数として用いられていた。

### シグモイド関数

**シグモイド関数**は、最小値が0で最大値が1となるようなS字曲線の関数で、最終的な確率を出力するような場合に有用。ただし、勾配が0に近い値となることが多く、学習がうまく進まないという問題がある。

### tanh関数

**tanh関数**（ハイパボリックタンジェント関数）は、最小値が-1で最大値が1となるようなS字曲線の関数。シグモイド関数に比べて勾配消失問題が軽減される。

### ReLU関数

**ReLU関数**(Rectified Linear Unit)は、負の入力は0として、それ以外の入力はそのまま出力する関数。勾配消失問題が起きにくいため、よく用いられている。


## ドロップアウト

**ドロップアウト**は、一定の確率で中間層の一部を無視して学習を行うことで、過学習を防止する手法。


## 転移学習

**転移学習**は、あるタスクに最適化されているニューラルネットワークモデルを、類似した別のタスクに応用するために調整する学習方法。転移学習がうまくいけば、学習に必要な時間が大幅に削減できる。

### ファインチューニング

**ファインチューニング**は、既に学習済みのニューラルネットワークモデルの後段の層のパラメータのみを学習し直し、タスクに最適化させる手法。

### 負の転移

**負の転移**は、転移学習において、元のモデルが解決しようとしていたタスクと転移先のタスクがそれほど似ていなかった場合に、通常より性能の悪いモデルが完成してしまうこと。

### ドメイン混合

**ドメイン混合**は、ニューラルネットワークにおいて本来特化させたいタスクとは別の出力を設けることで、モデルがあるタスクにフィッティングしすぎることを防ぐ手法。これにより、モデルを転移学習に利用しやすくする。

### マルチタスク学習

**マルチタスク学習**は、複数のタスクに関する学習を同時に行う手法。

### One-shot学習

**One-shot学習**は、分類問題において、あるラベルの訓練データがひとつ（あるいは小数）しかなくても正しく分類ができるような学習方法。

### Zero-shot学習

**Zero-shot学習**は、分類問題において、あるラベルの訓練データがひとつもなくても正しく分類ができるような学習方法。


## 特徴抽出

**特徴抽出**は、あるデータの特徴量から、情報量の少ない特徴量を削除したり、いくつかの特徴量からそれらを複合した新しい特徴量を合成したりする方法。ニューラルネットワークの出力層をそのほかのモデル（SVMなど）につなげることで、特徴抽出器としても用いることができる。


## パターン認識

**パターン認識**は、画像認識や音声認識といった、学習データから一定のパターンを見つけ出すことによって未知データを解析する方法。


## 自然言語処理

**自然言語処理**は、人が用いる言語をテキストとしてコンピュータに処理させること。

### 分散表現

**分散表現**は、ひとつの単語を低次元のベクトルとして表現することで、コンピュータで処理できるようにする手法。
